{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando Dataframes \n",
    "\n",
    "Nesse documento serão demonstradas as formas corretas e incorretas de se criar um DataFrame Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://guilhermeguim:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x188a8836690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Criando uma Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta situação são passados dois argumentos para o método createDataFrame. O primeiro é o dado e o segundo é uma lista com os nomes das colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nome</th>\n",
       "      <th>Idade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Nome Idade\n",
       "0  Hello    35\n",
       "1   Word    29"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('Hello','35'), ('Word', '29')]\n",
    "colNames = ['Nome', 'Idade']\n",
    "df = spark.createDataFrame(data, colNames)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código deste item passamos como argumento apenas os dados (uma lista de dicionários). Perceba que desta vez não passamos o segundo argumento pois neste caso as chaves dos dicionários dão os nomes das colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idade</th>\n",
       "      <th>Nome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>Hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>Word</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Idade   Nome\n",
       "0    35  Hello\n",
       "1    29   Word"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{'Nome': 'Hello', 'Idade': '35'}, {'Nome': 'Word', 'Idade': '29'}]\n",
    "df = spark.createDataFrame(data)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos criar DataFrames do Spark a partir de DataFrames do pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nome</th>\n",
       "      <th>Idade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zeca</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eva</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Nome Idade\n",
       "0  Zeca    35\n",
       "1   Eva    29"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [{'Nome': 'Zeca', 'Idade': '35'}, {'Nome': 'Eva', 'Idade': '29'}]\n",
    "pandas_df = pd.DataFrame(data)\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui o problema está em não passarmos este dicionário dentro de um iterável, como uma lista por exemplo. Isso faz com que o método não consiga inferir o esquema dos dados. Experimente passar o dicionário dentro de uma lista e observe o resultado obtido. Note que o DataFrame é criado, mas não conforme o esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `str`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m data = {\u001b[33m'\u001b[39m\u001b[33mNome\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mZeca\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mEva\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mIdade\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33m35\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m29\u001b[39m\u001b[33m'\u001b[39m]}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\session.py:1443\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd.DataFrame):\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m).createDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m   1441\u001b[39m         data, schema, samplingRatio, verifySchema\n\u001b[32m   1442\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\session.py:1485\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1483\u001b[39m     rdd, struct = \u001b[38;5;28mself\u001b[39m._createFromRDD(data.map(prepare), schema, samplingRatio)\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m     rdd, struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1487\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\session.py:1093\u001b[39m, in \u001b[36mSparkSession._createFromLocal\u001b[39m\u001b[34m(self, data, schema)\u001b[39m\n\u001b[32m   1090\u001b[39m     data = \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m     converter = _create_converter(struct)\n\u001b[32m   1095\u001b[39m     tupled_data: Iterable[Tuple] = \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\session.py:955\u001b[39m, in \u001b[36mSparkSession._inferSchemaFromList\u001b[39m\u001b[34m(self, data, names)\u001b[39m\n\u001b[32m    953\u001b[39m infer_array_from_first_element = \u001b[38;5;28mself\u001b[39m._jconf.legacyInferArrayTypeFromFirstElement()\n\u001b[32m    954\u001b[39m prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m schema = \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m    970\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    971\u001b[39m         message_parameters={},\n\u001b[32m    972\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\session.py:958\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    953\u001b[39m infer_array_from_first_element = \u001b[38;5;28mself\u001b[39m._jconf.legacyInferArrayTypeFromFirstElement()\n\u001b[32m    954\u001b[39m prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n\u001b[32m    955\u001b[39m schema = reduce(\n\u001b[32m    956\u001b[39m     _merge_type,\n\u001b[32m    957\u001b[39m     (\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m         \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[32m    966\u001b[39m     ),\n\u001b[32m    967\u001b[39m )\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m    970\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    971\u001b[39m         message_parameters={},\n\u001b[32m    972\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\spark-3.5.5-bin-hadoop3\\python\\pyspark\\sql\\types.py:1684\u001b[39m, in \u001b[36m_infer_schema\u001b[39m\u001b[34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[39m\n\u001b[32m   1681\u001b[39m     items = \u001b[38;5;28msorted\u001b[39m(row.\u001b[34m__dict__\u001b[39m.items())\n\u001b[32m   1683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1684\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1685\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1686\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33mdata_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1687\u001b[39m     )\n\u001b[32m   1689\u001b[39m fields = []\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `str`."
     ]
    }
   ],
   "source": [
    "data = {'Nome': ['Zeca', 'Eva'], 'Idade': ['35', '29']}\n",
    "df = spark.createDataFrame(data)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
